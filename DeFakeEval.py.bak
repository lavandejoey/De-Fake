from __future__ import annotations
import argparse, os, time, logging
from pathlib import Path
from typing import Dict, Any, List, Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms as T
from PIL import Image
import pandas as pd
import numpy as np
import clip

from DataUtils import (
    index_dataframe, REQUIRED_COLS, standardise_predictions, FakePartsV2DatasetBase,
)

from blipmodels import blip_decoder
from torch.utils.data._utils.collate import default_collate

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
log = logging.getLogger(__file__)


class NeuralNet(nn.Module):
    def __init__(self, input_size: int, hidden: Tuple[int, int] = (512, 256), num_classes: int = 2):
        super().__init__()
        self.fc1 = nn.Linear(input_size, hidden[0])
        self.fc2 = nn.Linear(hidden[0], hidden[1])
        self.fc3 = nn.Linear(hidden[1], num_classes)
        self.dropout = nn.Dropout(0.5)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        return self.fc3(x)


try:
    # Allowlist your head class for newer safe loading paths
    from torch.serialization import add_safe_globals

    add_safe_globals([NeuralNet])
except Exception:
    pass


def load_blip(device: str, image_size: int = 224) -> nn.Module:
    url = "https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth"
    model = blip_decoder(pretrained=url, image_size=image_size, vit="base").to(device)
    model.eval()
    return model


def collate_batch(batch: List[Tuple[torch.Tensor, torch.Tensor, Dict[str, Any]]]):
    """Keep metas as list[dict] (donâ€™t auto-collate to dict[str, list])."""
    blip_tensors = default_collate([b[0] for b in batch])  # [B,3,H,W]
    clip_tensors = default_collate([b[1] for b in batch])  # [B,3,H,W]
    metas = [b[2] for b in batch]
    return blip_tensors, clip_tensors, metas


class FakePartsV2Dataset(FakePartsV2DatasetBase):
    """
    Loads a PIL image once, emits:
      - blip_tensor: for BLIP captioning
      - clip_tensor: for CLIP encoding
      - meta dict (task/method/subset/label/mode/paths + error flag)
    On error, returns zero tensors + error=True (keeps batch shape stable).
    """

    def __init__(self,
                 data_root: str | Path,
                 clip_preprocess,
                 blip_size: int = 224,
                 csv_path: str | Path | None = None,
                 model_name: str = "DeFake",
                 on_corrupt: str = "warn"):
        super().__init__(
            data_root=data_root,
            mode="frame",
            csv_path=csv_path,
            model_name=model_name,
            on_corrupt=on_corrupt
        )
        self.clip_preprocess = clip_preprocess
        self.blip_transform = T.Compose([
            T.Resize(blip_size),
            T.CenterCrop(blip_size),
            T.ToTensor(),
        ])

    def __getitem__(self, idx: int):
        abs_path = self._abs_paths[idx]
        label = int(self._labels[idx])
        error = False
        try:
            with Image.open(abs_path) as im:
                im = im.convert("RGB")
                im.load()
            blip_tensor = self.blip_transform(im)
            clip_tensor = self.clip_preprocess(im)
        except Exception as e:
            log.debug("Image load failed (%s): %s", abs_path, e)
            blip_tensor = torch.zeros(3, 224, 224)
            clip_tensor = torch.zeros(3, 224, 224)
            error = True

        meta = self._make_meta(idx, label)
        meta.update({
            "abs_path": str(abs_path),
            "rel_path": self._rel_paths[idx],
            "error": error,
        })
        return blip_tensor, clip_tensor, meta


@torch.no_grad()
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--data_root", required=True, type=str, help="Dataset root")
    ap.add_argument("--data_csv", type=str, default=None, help="Optional prebuilt index CSV")
    ap.add_argument("--results", required=True, type=str, help="Folder to save predictions.csv")
    ap.add_argument("--clip_name", default="ViT-B/32", type=str, help="openai/CLIP backbone name")
    ap.add_argument("--clip_model_ckpt", default="finetune_clip.pt", type=str, help="(optional) finetuned CLIP")
    ap.add_argument("--linear_head_ckpt", default="clip_linear.pt", type=str, help="MLP head ckpt")
    ap.add_argument("--batch_size", type=int, default=64)
    ap.add_argument("--workers", type=int, default=max(1, (os.cpu_count() or 4) // 2))
    ap.add_argument("--fp16", action="store_true", help="Enable mixed precision on CUDA")
    args = ap.parse_args()

    device = "cuda" if torch.cuda.is_available() else "cpu"
    use_amp = bool(args.fp16 and device == "cuda")
    out_dir = Path(args.results);
    out_dir.mkdir(parents=True, exist_ok=True)
    out_csv = out_dir / "predictions.csv"

    # 1) Index
    log.info("Indexing: root=%s, csv=%s", args.data_root, args.data_csv or "None")
    t0 = time.time()
    df_idx = index_dataframe(Path(args.data_root), csv_path=args.data_csv)
    if len(df_idx) == 0:
        raise SystemExit(f"No files found under {args.data_root}")
    log.info("Indexed %d files in %.2fs", len(df_idx), time.time() - t0)

    # 2) Models
    log.info("Loading BLIP and CLIP (%s)", args.clip_name)
    blip = load_blip(device, image_size=224)
    clip_model, clip_preprocess = clip.load(args.clip_name, device=device)
    clip_model.eval()

    # Optional: finetuned CLIP backbone (best-effort)
    if os.path.exists(args.clip_model_ckpt):
        try:
            # Explicitly disable weights_only to permit class unpickling if needed
            try:
                maybe_model = torch.load(args.clip_model_ckpt, map_location=device, weights_only=False)
            except TypeError:
                # older torch doesn't have weights_only kwarg
                maybe_model = torch.load(args.clip_model_ckpt, map_location=device)

            if isinstance(maybe_model, nn.Module):
                clip_model = maybe_model.to(device).eval()
                log.info("Loaded finetuned CLIP from %s", args.clip_model_ckpt)
        except Exception as e:
            log.warning("Could not load finetuned CLIP (%s): %s", args.clip_model_ckpt, e)

    # Head: allow raw state_dict or wrapper dict with 'state_dict'
    with torch.cuda.amp.autocast(enabled=False):
        # Determine feature dims from CLIP
        img_dim = getattr(clip_model.visual, "output_dim", 512)
        txt_dim = getattr(clip_model.transformer, "width", 512)
    head_in = int(img_dim + txt_dim)
    head = NeuralNet(head_in).to(device).eval()

    try:
        # Force legacy behaviour so pickled objects (e.g., whole module) can load
        try:
            obj = torch.load(args.linear_head_ckpt, map_location=device, weights_only=False)
        except TypeError:
            obj = torch.load(args.linear_head_ckpt, map_location=device)

        state = obj["state_dict"] if isinstance(obj, dict) and "state_dict" in obj else obj
        head.load_state_dict(state, strict=False)
        log.info("Loaded head from %s", args.linear_head_ckpt)
    except Exception as e:
        log.error("Failed to load linear head (%s): %s", args.linear_head_ckpt, e)
        raise

    # 3) DataLoader
    ds = FakePartsV2Dataset(
        data_root=args.data_root,
        clip_preprocess=clip_preprocess,
        blip_size=224,
        csv_path=args.data_csv,
        model_name=f"DeFake-{args.clip_name}"
    )
    loader = DataLoader(
        ds, batch_size=args.batch_size, num_workers=args.workers, pin_memory=(device == "cuda"),
        shuffle=False, drop_last=False, collate_fn=collate_batch,
        persistent_workers=(args.workers or 0) > 0,
    )
    log.info("DataLoader: bs=%d, workers=%d", args.batch_size, args.workers)

    # 4) Inference
    rows: List[Dict[str, Any]] = []
    torch.backends.cudnn.benchmark = True
    seen, failed = 0, 0
    t1 = time.time()

    for blip_batch, clip_batch, metas in loader:
        B = len(metas)
        failed_mask = torch.tensor([m["error"] for m in metas], dtype=torch.bool)
        failed += int(failed_mask.sum().item())
        seen += B

        # Device moves
        blip_imgs = blip_batch.to(device, non_blocking=True)
        clip_imgs = clip_batch.to(device, non_blocking=True)

        try:
            # 1) captions (batched)
            with torch.cuda.amp.autocast(enabled=use_amp):
                captions = blip.generate(blip_imgs, sample=False, num_beams=3, max_length=60, min_length=5)

            if isinstance(captions, str):
                captions = [captions]
            elif not isinstance(captions, (list, tuple)):
                captions = list(captions)
            if len(captions) != B:
                captions = (list(captions) + ["photo"] * B)[:B]
            captions = [c if isinstance(c, str) and c.strip() else "photo" for c in captions]

            # 2) CLIP encodings + head
            tokens = clip.tokenize(captions, truncate=True).to(device)
            with torch.cuda.amp.autocast(enabled=use_amp):
                img_feat = F.normalize(clip_model.encode_image(clip_imgs).float(), dim=1)
                txt_feat = F.normalize(clip_model.encode_text(tokens).float(), dim=1)
                logits = head(torch.cat([img_feat, txt_feat], dim=1).float())  # [B,2]
                probs = F.softmax(logits, dim=1)[:, 1]  # P(fake)
                preds = torch.argmax(logits, dim=1)

            # 3) build rows
            for i, m in enumerate(metas):
                score = -1.0 if m["error"] else float(probs[i].item())
                pred = 0 if m["error"] else int(preds[i].item())
                rows.append({
                    "sample_id": m["rel_path"],
                    "task": m["task"],
                    "method": m["method"],
                    "subset": m["subset"],
                    "label": int(m["label"]),
                    "model": f"DeFake-{args.clip_name}",
                    "mode": m["mode"],
                    "score": score,
                    "pred": pred,
                })

        except Exception as e:
            # Extremely rare (e.g., caption decode edge-case): emit neutral rows
            log.warning("Batch failed, emitting neutral rows: %s", e)
            for m in metas:
                rows.append({
                    "sample_id": m["rel_path"], "task": m["task"], "method": m["method"],
                    "subset": m["subset"], "label": int(m["label"]),
                    "model": f"DeFake-{args.clip_name}", "mode": m["mode"],
                    "score": -1.0, "pred": 0,
                })

    dt = time.time() - t1
    ips = seen / dt if dt > 0 else 0.0
    log.info("Inference: %d samples in %.1fs (%.1f img/s), failed=%d", seen, dt, ips, failed)

    # 5) Save CSV (schema hygiene)
    df_out = standardise_predictions(rows).loc[:, list(REQUIRED_COLS)]
    out_csv.parent.mkdir(parents=True, exist_ok=True)
    df_out.to_csv(out_csv, index=False)
    log.info("Saved %s (%d rows, cols=%s)", out_csv, len(df_out), list(df_out.columns))


if __name__ == "__main__":
    main()
